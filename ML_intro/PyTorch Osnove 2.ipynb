{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPqynHVpo75eKK4AafL4iYP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **PyTorch Osnove 2: Razumevanje Modularnog Sistema PyTorch-a**\n","\n","**Kurs:** KSMF1  \n","**Trajanje:** ~45 min  \n","**Preduslovi:** Osnove PyTorch tenzora (PyTorch Osnove 1)\n","\n","---"],"metadata":{"id":"_tUUiYuVQTwh"}},{"cell_type":"markdown","source":["## Ciljevi Sekcije\n","\n","Do kraja ove sekcije, naučićete da:\n","\n","- Razumete da je sve u PyTorch-u (slojevi, aktivacije, gubici) `nn.Module`\n","- Koristite **ugrađene** module kao što su `nn.Linear`, `nn.ReLU`, i `nn.MSELoss`\n","  - Izgradite jednostavne neuronske mreže kombinovanjem **ugrađenih** modula\n","- Kreirate **prilagođene** module od nule\n","  - Izgradite jednostavne neuronske mreže kombinovanjem **prilagođenih** modula\n","- Razumete konzistentan obrazac koji sve PyTorch komponente prate\n","---"],"metadata":{"id":"F07O87i4Qf-x"}},{"cell_type":"markdown","source":["# **Deo 3: Uvod u PyTorch Module**"],"metadata":{"id":"393udRm1Qk7d"}},{"cell_type":"markdown","source":["## 3.1 Šta je Modul?\n","\n","U PyTorch-u, **modul** je gradivni blok neuronske mreže (modela).\n","\n","Posmatrajte PyTorch module kao **LEGO kockice** za gradnju neuronskih mreža. Baš kao što možete da kombinujete jednostavne LEGO delove da biste izgradili složene strukture, možete kombinovati jednostavne komponente neuronskih mreža da biste izgradili sofisticirane modele.\n","\n","Ti gradivni blokovi su najčešće \"koraci obrade\" koji uzimaju neki ulaz x i vraćaju izlaz f(x). U ovom kontekstu, možemo videti da je \"potpuno povezani sloj\" modul, aktivaciona funkcija je takođe modul, čak su i funkcije gubitka moduli!"],"metadata":{"id":"OHFT3IinQx4V"}},{"cell_type":"code","source":["# Učitajmo PyTorch i druge važne biblioteke\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Postavi random seed za reproducibilnost\n","torch.manual_seed(42)\n","print(\"PyTorch version:\", torch.__version__)"],"metadata":{"id":"uDbabd4YQ4lE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"KVdEXUpnRKWH"}},{"cell_type":"markdown","source":["## 3.2 Osnovna klasa: **`nn.Module`**\n","\n","Svaka komponenta neuronske mreže u PyTorch-u je podklasa (nasleđuje od) `nn.Module`. Ne brinite ako niste upoznati sa klasama - posmatrajte `nn.Module` kao **šablon** koji pruža zajedničku funkcionalnost za sve gradivne blokove neuronskih mreža.\n","\n","U PyTorch-u, možete definisati svoje **prilagođene module**, ali on takođe dolazi sa velikim brojem već **ugrađenih modula** za lako i brzo implementiranje. Svi oni su **podklase** od `nn.Module`."],"metadata":{"id":"tHkJUc8yRLdg"}},{"cell_type":"markdown","source":["### Šta `nn.Module` Pruža"],"metadata":{"id":"Bp2ad0sIRWRN"}},{"cell_type":"code","source":["# Hajde da pogledamo šta nn.Module pruža\n","print(\"Methods available in nn.Module:\")\n","module_methods = [method for method in dir(nn.Module) if not method.startswith('_')]\n","print(f\"Total methods: {len(module_methods)}\")\n","print(\"List of methods:\", module_methods)"],"metadata":{"id":"MHr-3ilhRfNS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Najvažnije Metode"],"metadata":{"id":"XdGb_jHLRiWG"}},{"cell_type":"markdown","source":["Ključne metode koje ćemo koristiti:\n","\n","  - **forward():** Definiše kako podaci prolaze kroz modul (definiše **f(x; w)**)\n","  - **parameters():** Vraća sve parametre **w** koji se mogu trenirati\n","  - **train():** Postavlja modul u režim treniranja\n","  - **eval()**: Postavlja modul u režim evaluacije"],"metadata":{"id":"vFDLVHEy1tnT"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"eTc1A9bwRxzx"}},{"cell_type":"markdown","source":["## 3.3 Ugrađeni (Built-in) Moduli\n","\n","PyTorch dolazi sa velikim brojem već definisanih modula za gradnju neuronskih mreža. Nazivamo ih **ugrađeni slojevi** i oni uključuju skoro sve vrste slojeva za obradu, aktivacionih slojeva i funkcija gubitka koje se često koriste.\n","\n","Pogledajmo neke primere:"],"metadata":{"id":"QFlQzlpgSCDp"}},{"cell_type":"markdown","source":["### Primer 1 - Linearni Slojevi (Potpuno Povezani Slojevi)\n","\n","Linearni sloj izvršava operaciju: **izlaz = težine × ulaz + pomeraj**, gde je ulaz vektor veličine N, težine su MxN matrica, a pomeraj je vektor veličine M. Ovo čini izlazni vektor veličine M.\n","\n","Ovo je bukvalno jednačina prave: **f(x) = wx + b**, ali proširena na više dimenzija (jednačina hiper-ravni)!"],"metadata":{"id":"QsRBd7tNS_Nh"}},{"cell_type":"code","source":["# Kreirajmo linearni sloj koristeći ugrađenu klasu nn.Linear\n","# Ulazne karakteristike: 3 (npr., x, y, z koordinate)\n","# Izlazne karakteristike: 2 (npr., energija, impuls)\n","\n","linear = nn.Linear(in_features=3, out_features=2)\n","\n","# Ispitaj parametre\n","print(f\"\\nParameters:\")\n","print(f\"Weight shape: {linear.weight.shape}\")  # (out_features, in_features)\n","print(f\"Bias shape: {linear.bias.shape}\")      # (out_features,)\n","print(f\"Weight matrix:\\n{linear.weight.data}\")\n","print(f\"Bias vector: {linear.bias.data}\")\n","\n","# Koristi sloj\n","input_data = torch.randn(4, 3)  # Kreiraj izmišljeni batch podataka od 4 uzorka, 3 karakteristike svaki\n","output = linear(input_data)  # Ovde koristimo sloj kao funkciju\n","\n","print(f\"\\nUsage:\")\n","print(f\"Input shape: {input_data.shape}\")\n","print(f\"Output shape: {output.shape}\")\n","print(f\"Input:\\n{input_data}\")\n","print(f\"Output:\\n{output}\")\n","\n","# Proveri izračun ručno\n","manual_output = torch.matmul(input_data, linear.weight.t()) + linear.bias\n","print(f\"\\nManual computation matches PyTorch: {torch.allclose(output, manual_output)}\")"],"metadata":{"id":"appIHp5FTAU6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Primer 2 - Aktivacione Funkcije\n","\n","Aktivacione funkcije uvode **nelinearnost** u neuronske mreže. Bez njih, višestruki linearni slojevi bi bili ekvivalentni jednom linearnom sloju!\n","\n","Ispitajmo neke od najčešćih aktivacionih funkcija:"],"metadata":{"id":"6Hb92Zx8TLxH"}},{"cell_type":"code","source":["# Pripremi x-osu za vizualizaciju\n","x = torch.linspace(-3, 3, 100)\n","\n","# ReLU (Rectified Linear Unit): f(x) = max(0, x)\n","relu = nn.ReLU()\n","relu_output = relu(x)\n","\n","# Sigmoid: f(x) = 1 / (1 + e^(-x))\n","sigmoid = nn.Sigmoid()\n","sigmoid_output = sigmoid(x)\n","\n","# Tanh: f(x) = tanh(x)\n","tanh = nn.Tanh()\n","tanh_output = tanh(x)\n","\n","# Nacrtaj aktivacione funkcije\n","plt.figure(figsize=(12, 4))\n","\n","plt.subplot(1, 3, 1)\n","plt.plot(x.numpy(), relu_output.numpy(), 'r-', linewidth=2)\n","plt.title('ReLU Activation')\n","plt.grid(True)\n","plt.xlabel('Input')\n","plt.ylabel('Output')\n","\n","plt.subplot(1, 3, 2)\n","plt.plot(x.numpy(), sigmoid_output.numpy(), 'g-', linewidth=2)\n","plt.title('Sigmoid Activation')\n","plt.grid(True)\n","plt.xlabel('Input')\n","plt.ylabel('Output')\n","\n","plt.subplot(1, 3, 3)\n","plt.plot(x.numpy(), tanh_output.numpy(), 'b-', linewidth=2)\n","plt.title('Tanh Activation')\n","plt.grid(True)\n","plt.xlabel('Input')\n","plt.ylabel('Output')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"ReLU: Popularan jer je jednostavan i dobro radi\")\n","print(\"Sigmoid: Izlazi između 0 i 1, dobar za verovatnoće\")\n","print(\"Tanh: Izlazi između -1 i 1, centriran na nuli\")"],"metadata":{"id":"M32ySLA_TOEc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Primer 3 - Funkcije Gubitka\n","\n","Funkcije gubitka mere koliko su predviđanja modela pogrešna tako što uzimaju naš finalni izlaz i poznati rezultat iz dataset-a kao svoje ulaze, i primenjuju neku metriku da izmere koliko su različiti.\n","\n","**Funkcije gubitka u PyTorch-u su takođe nn.Module-i!** To znači da prate iste obrasce koje smo učili."],"metadata":{"id":"LE9qfsqmTT2C"}},{"cell_type":"code","source":["# Pogledajmo različite tipove funkcija gubitka\n","print(\"\\n=== Common Loss Functions ===\")\n","\n","# Za regresiju (predviđanje kontinuiranih vrednosti)\n","print(\"Regression losses:\")\n","print(\"- nn.MSELoss: Mean Squared Error\")\n","print(\"- nn.L1Loss: Mean Absolute Error\")\n","print(\"- nn.SmoothL1Loss: Huber loss (otporan na izuzetke)\")\n","\n","# Za klasifikaciju (predviđanje kategorija)\n","print(\"\\nClassification losses:\")\n","print(\"- nn.CrossEntropyLoss: Za multiklasnu klasifikaciju\")\n","print(\"- nn.BCELoss: Binary Cross Entropy za binarnu klasifikaciju\")\n","print(\"- nn.NLLLoss: Negative Log Likelihood\")\n","\n","# Kreiranje instanci\n","mse_loss = nn.MSELoss()\n","cross_entropy_loss = nn.CrossEntropyLoss()"],"metadata":{"id":"KdYLsj0pTVyx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Razumevanje Različitih Funkcija Gubitka"],"metadata":{"id":"fdD6UXgeTZpl"}},{"cell_type":"code","source":["# Uporedimo različite funkcije gubitka na primerima\n","print(\"=== Loss Function Comparison ===\")\n","\n","# Primer regresije\n","predicted_energy = torch.tensor([10.2, 15.8, 8.1])\n","actual_energy = torch.tensor([10.0, 16.0, 8.5])\n","\n","mse = nn.MSELoss()(predicted_energy, actual_energy)\n","mae = nn.L1Loss()(predicted_energy, actual_energy)\n","smooth_l1 = nn.SmoothL1Loss()(predicted_energy, actual_energy)\n","\n","print(\"Regression losses:\")\n","print(f\"Predictions: {predicted_energy}\")\n","print(f\"Actual: {actual_energy}\")\n","print(f\"MSE Loss: {mse:.4f}\")\n","print(f\"MAE Loss: {mae:.4f}\")\n","print(f\"Smooth L1 Loss: {smooth_l1:.4f}\")\n","\n","# Primer klasifikacije\n","raw_scores = torch.tensor([\n","    [2.1, 0.5, -1.2],  # Snažno predviđa klasu 0\n","    [-0.8, 1.9, 0.1],  # Snažno predviđa klasu 1\n","    [0.2, -1.1, 1.8]   # Snažno predviđa klasu 2\n","])\n","true_labels = torch.tensor([0, 1, 2])\n","\n","cross_entropy = nn.CrossEntropyLoss()(raw_scores, true_labels)\n","\n","print(f\"\\nClassification loss:\")\n","print(f\"Raw scores:\\n{raw_scores}\")\n","print(f\"True labels: {true_labels}\")\n","print(f\"CrossEntropy Loss: {cross_entropy:.4f}\")"],"metadata":{"id":"yTX_-M1kTcm0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"FagFTRMUT7jB"}},{"cell_type":"markdown","source":["## 3.4 Gradnja Neuronske Mreže sa Ugrađenim Modulima\n","\n","Hajde da iskombinujemo ugrađene module da izgradimo jednostavnu neuronsku mrežu!\n","\n","**VEOMA VAŽNO**: Baš kao što slojevi uzimaju ulaz x i vraćaju izlaz f(x), isto to radi i cela mreža. To znači da je **cela mreža zapravo `nn.Module`!**\n","\n","Ovo nam omogućava da koristimo \"slojeve apstrakcije\", tj. da koristimo naše mreže kao gradivne blokove za još složenije strukture.\n","\n","Postoje dva načina da se iskombinuju moduli u jedan veći modul (ili punu mrežu):\n","1. Korišćenje `nn.Sequential`: Ovo je modul koji automatski slaže module jedan za drugim. Pojednostavljuje gradnju mreža gde slojevi dolaze u sekvenci.\n","2. Definisanje većeg modula kao **prilagođeni modul**: Ovo je fleksibilniji metod za nesekvencijalno slaganje modula. Naučićemo kako da pravimo prilagođene module u sledećem odeljku."],"metadata":{"id":"XLMygTspT8vz"}},{"cell_type":"markdown","source":["### Korišćenje `nn.Sequential`"],"metadata":{"id":"A4UuWFh-UZqs"}},{"cell_type":"code","source":["# Metod 1: Korišćenje nn.Sequential (automatski slažemo naše slojeve)\n","simple_network = nn.Sequential(\n","    nn.Linear(4, 8),     # 4 ulazne karakteristike → 8 skrivenih jedinica\n","    nn.ReLU(),           # Aktivacijska funkcija\n","    nn.Linear(8, 4),     # 8 skrivenih jedinica → 4 skrivene jedinice\n","    nn.ReLU(),           # Još jedna aktivacija\n","    nn.Linear(4, 1)      # 4 skrivene jedinice → 1 izlaz\n",")\n","\n","print(\"=== Simple Neural Network ===\")\n","print(\"Network architecture:\")\n","print(simple_network)\n","print(f\"\\nThe network is an nn.Module: {isinstance(simple_network, nn.Module)}\")\n","\n","# Testiraj mrežu\n","test_input = torch.randn(3, 4)  # 3 uzorka, 4 karakteristike svaki\n","network_output = simple_network(test_input)\n","\n","print(f\"\\nTesting the network:\")\n","print(f\"Input shape: {test_input.shape}\")\n","print(f\"Output shape: {network_output.shape}\")\n","print(f\"Input:\\n{test_input}\")\n","print(f\"Output:\\n{network_output}\")\n","\n","# Prebroj ukupne parametre\n","total_params = sum(p.numel() for p in simple_network.parameters())\n","print(f\"\\nTotal parameters: {total_params}\")\n","\n","# Detaljni pregled parametara\n","print(\"\\nParameter breakdown:\")\n","for i, layer in enumerate(simple_network):\n","    if hasattr(layer, 'weight'):\n","        weight_params = layer.weight.numel()\n","        bias_params = layer.bias.numel() if layer.bias is not None else 0\n","        total_layer_params = weight_params + bias_params\n","        print(f\"Layer {i} ({layer.__class__.__name__}): {total_layer_params} parameters\")\n","        print(f\"  - Weights: {layer.weight.shape} = {weight_params} params\")\n","        print(f\"  - Bias: {layer.bias.shape} = {bias_params} params\")\n","    else:\n","        print(f\"Layer {i} ({layer.__class__.__name__}): 0 parameters\")"],"metadata":{"id":"5zd8bixvUFnB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Testiranje sa Funkcijom Gubitka"],"metadata":{"id":"BxKOrdkEUdPi"}},{"cell_type":"code","source":["# Hajde da koristimo našu mrežu sa funkcijom gubitka\n","print(\"=== Network + Loss Function ===\")\n","\n","# Kreiraj neke lažne ciljne podatke za naš test\n","test_targets = torch.randn(3, 1)  # 3 uzorka, 1 cilj svaki\n","loss_function = nn.MSELoss()\n","\n","# Izračunaj gubitak\n","loss = loss_function(network_output, test_targets)\n","\n","print(f\"Network predictions:\\n{network_output}\")\n","print(f\"Target values:\\n{test_targets}\")\n","print(f\"MSE Loss: {loss:.4f}\")"],"metadata":{"id":"FMMpJr4hUfaA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"YJeje2UKUiCN"}},{"cell_type":"markdown","source":["## 3.5 Prilagođeni Moduli\n","\n","Prava snaga PyTorch-a leži u **kreiranju vaših sopstvenih prilagođenih modula**. Da bismo razumeli kako ovo stvarno funkcioniše, ponovo ćemo kreirati neke od modula koje smo videli u prethodnom odeljku, ali ovog puta od nule.\n","\n","U sledećim primerima, gradićemo `Linear layer`, `ReLU activation layer`, i `MSE loss function` kao potklase `nn.Modul`-a:"],"metadata":{"id":"bFasM7ySUjV1"}},{"cell_type":"markdown","source":["### Primer 1 - Prilagođeni Linearni Slojevi\n","\n","Hajde da kreiramo `nn.Linear` sloj sami. Ovo će nam pokazati kako ugrađeni slojevi rade \"ispod haube\"."],"metadata":{"id":"sgJ1fQb4U0vb"}},{"cell_type":"code","source":["class CustomLinear(nn.Module):\n","    \"\"\"\n","    Naša sopstvena implementacija nn.Linear da bismo razumeli kako radi interno.\n","    Ovo je kao pravljenje sopstvenog voltmetra umesto kupovine već napravljenog -\n","    razumete tačno kako radi!\n","    \"\"\"\n","    def __init__(self, in_features, out_features, bias=True):\n","        super(CustomLinear, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","\n","        # Kreiraj matricu težina - ovo je ono što će mreža učiti!\n","        # BITNO: Moramo da je definišemo kao nn.Parameter da bi PyTorch znao da su to vrednosti koje treba da ažurira pri treningu\n","        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n","\n","        if bias:\n","            self.bias = nn.Parameter(torch.randn(out_features))\n","        else:\n","            self.bias = None\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass: izračunaj output = input @ weight.T + bias\n","        Ovo je isto kao: output = weight @ input + bias za svaki uzorak\n","        \"\"\"\n","        # Matrično množenje: (batch_size, in_features) @ (in_features, out_features)\n","        output = torch.matmul(x, self.weight.t())  # .t() znači transpozicija\n","\n","        if self.bias is not None:\n","            output = output + self.bias\n","\n","        return output\n","\n","# Poredimo naš prilagođeni sloj sa PyTorch-ovim ugrađenim slojem\n","\n","# Fiksiraj seed zbog reprudicibilnosti\n","torch.manual_seed(42)\n","\n","# Kreiraj oba sloja sa istim dimenzijama\n","builtin_layer = nn.Linear(3, 2)\n","custom_layer = CustomLinear(3, 2)\n","\n","# Kopiraj težine da budu identične za poređenje\n","custom_layer.weight.data = builtin_layer.weight.data.clone()\n","custom_layer.bias.data = builtin_layer.bias.data.clone()\n","\n","# Testiraj na istim ulaznim podacima\n","test_input = torch.randn(4, 3)\n","builtin_output = builtin_layer(test_input)\n","custom_output = custom_layer(test_input)\n","\n","print(\"Comparing built-in vs custom linear layer:\")\n","print(f\"Built-in output:\\n{builtin_output}\")\n","print(f\"Custom output:\\n{custom_output}\")\n","print(f\"Maximum difference: {torch.abs(builtin_output - custom_output).max():.10f}\")\n","print(\"They're identical! Our custom layer works perfectly.\")"],"metadata":{"id":"D1QzK0yLU23G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Primer 2 - Prilagođeni ReLU aktivacijski sloj\n","\n","Baš kao i pre, hajde da izgradimo ReLU Aktivacioni Sloj od nule:"],"metadata":{"id":"vevCN0i0VD58"}},{"cell_type":"code","source":["class CustomReLU(nn.Module):\n","    \"\"\"\n","    Naša sopstvena implementacija nn.ReLU.\n","    ReLU(x) = max(0, x)\n","    \"\"\"\n","    def __init__(self):\n","        super(CustomReLU, self).__init__()\n","        # Aktivacione funkcije nemaju parametre za učenje!\n","\n","    def forward(self, x):\n","        \"\"\"Primeni ReLU aktivaciju: f(x) = max(0, x)\"\"\"\n","        return torch.clamp(x, min=0.0)  # Ekvivalentno sa torch.max(x, torch.zeros_like(x))\n","\n","# Testiraj naš prilagođeni ReLU\n","print(\"=== Custom ReLU Activation ===\")\n","custom_relu = CustomReLU()\n","builtin_relu = nn.ReLU()\n","\n","print(f\"Custom ReLU: {custom_relu}\")\n","print(f\"Built-in ReLU: {builtin_relu}\")\n","\n","# Testiraj sa pozitivnim i negativnim vrednostima\n","test_values = torch.tensor([-3.0, -1.0, 0.0, 1.0, 3.0])\n","custom_relu_output = custom_relu(test_values)\n","builtin_relu_output = builtin_relu(test_values)\n","\n","print(f\"\\nTesting ReLU:\")\n","print(f\"Input: {test_values}\")\n","print(f\"Custom ReLU: {custom_relu_output}\")\n","print(f\"Built-in ReLU: {builtin_relu_output}\")\n","print(f\"Outputs are identical: {torch.allclose(custom_relu_output, builtin_relu_output)}\")\n","\n","# Proveri da nema parametre\n","print(f\"\\nCustom ReLU parameters: {len(list(custom_relu.parameters()))}\")\n","print(f\"Built-in ReLU parameters: {len(list(builtin_relu.parameters()))}\")\n","print(f\"Both have 0 parameters (activations don't learn!)\")\n","\n","# Vizuelno poređenje\n","x = torch.linspace(-3, 3, 100)\n","custom_y = custom_relu(x)\n","builtin_y = builtin_relu(x)\n","\n","plt.figure(figsize=(8, 5))\n","plt.plot(x.numpy(), custom_y.numpy(), 'r-', linewidth=3, label='Custom ReLU', alpha=0.7)\n","plt.plot(x.numpy(), builtin_y.numpy(), 'b--', linewidth=2, label='Built-in ReLU')\n","plt.xlabel('Input')\n","plt.ylabel('Output')\n","plt.title('Custom vs Built-in ReLU')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","print(\"The curves are identical - our custom ReLU works perfectly!\")"],"metadata":{"id":"vhUhAIBkVEYU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Primer 3 - Prilagođena MSE Funkcija Gubitka\n","\n","Kreirajmo našu sopstvenu MSE funkciju gubitka da bismo razumeli kako funkcije gubitka rade interno:"],"metadata":{"id":"n4qXgwf2VItg"}},{"cell_type":"code","source":["class CustomMSELoss(nn.Module):\n","    \"\"\"\n","    Naša sopstvena implementacija Mean Squared Error gubitka.\n","    MSE = mean((predicted - actual)²)\n","    \"\"\"\n","    def __init__(self, reduction='mean'):\n","        super(CustomMSELoss, self).__init__()\n","        self.reduction = reduction\n","\n","    def forward(self, predicted, actual):\n","        \"\"\"Izračunaj MSE gubitak.\"\"\"\n","        # Izračunaj kvadratne razlike\n","        squared_diff = (predicted - actual) ** 2\n","\n","        # Primeni redukciju\n","        if self.reduction == 'mean':\n","            loss = torch.mean(squared_diff)\n","        elif self.reduction == 'sum':\n","            loss = torch.sum(squared_diff)\n","        elif self.reduction == 'none':\n","            loss = squared_diff\n","        else:\n","            raise ValueError(f\"Invalid reduction: {self.reduction}\")\n","\n","        return loss\n","\n","# Testiraj naš prilagođeni MSE\n","custom_mse = CustomMSELoss()\n","builtin_mse = nn.MSELoss()\n","\n","# Test podaci\n","pred = torch.tensor([1.2, 2.8, 3.1])\n","actual = torch.tensor([1.0, 3.0, 3.5])\n","\n","custom_loss = custom_mse(pred, actual)\n","builtin_loss = builtin_mse(pred, actual)\n","\n","print(\"Comparing custom vs built-in MSE loss:\")\n","print(f\"Custom MSE: {custom_loss:.6f}\")\n","print(f\"Built-in MSE: {builtin_loss:.6f}\")\n","print(f\"Difference: {torch.abs(custom_loss - builtin_loss):.10f}\")\n","\n","# Ručno računanje za verifikaciju\n","manual_mse = torch.mean((pred - actual) ** 2)\n","print(f\"Manual calculation: {manual_mse:.6f}\")"],"metadata":{"id":"j6XG0RdRVK3n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"LVIOiITNVNQc"}},{"cell_type":"markdown","source":["## 3.6 Gradnja Neuronske Mreže sa Prilagođenim Modulima\n","\n","Hajde sada da ponovo izgradimo našu neuronsku mrežu iz odeljka 3.4 koristeći samo prilagođene module koje smo napravili.\n","\n","Ovaj put, umesto korišćenja nn.Sequential da brzo kombinujemo slojeve, kreiračemo našu sopstvenu klasu prilagođene mreže (zapamtite da su mreže takođe moduli, samo složeniji)."],"metadata":{"id":"HzOH3dcFVN-w"}},{"cell_type":"markdown","source":["### Neuronska Mreža kao Prilagođeni `nn.Module`"],"metadata":{"id":"Jn_XjkQwVnaB"}},{"cell_type":"code","source":["class CustomNetwork(nn.Module):\n","    \"\"\"\n","    Prilagođena neuronska mreža izgrađena u potpunosti od naših prilagođenih modula.\n","    Ovo pokazuje kako se kreiraju složene mreže kombinovanjem jednostavnijih modula.\n","    \"\"\"\n","    def __init__(self, input_size=4, hidden_size1=8, hidden_size2=4, output_size=1):\n","        super(CustomNetwork, self).__init__()\n","\n","        # Definiši sve naše slojeve koristeći naše prilagođene module\n","        self.layer1 = CustomLinear(input_size, hidden_size1)\n","        self.activation1 = CustomReLU()\n","        self.layer2 = CustomLinear(hidden_size1, hidden_size2)\n","        self.activation2 = CustomReLU()\n","        self.layer3 = CustomLinear(hidden_size2, output_size)\n","\n","        # Čuvaj informacije o arhitekturi\n","        self.architecture = f\"{input_size}→{hidden_size1}→{hidden_size2}→{output_size}\"\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Definiši kako podaci prolaze kroz mrežu.\n","        Ovde specificiramo tačnu putanju izračunavanja.\n","        \"\"\"\n","        # Forward pass kroz svaki sloj\n","        x = self.layer1(x)      # Linearna transformacija\n","        x = self.activation1(x)  # Primeni ReLU\n","        x = self.layer2(x)      # Još jedna linearna transformacija\n","        x = self.activation2(x)  # Još jedan ReLU\n","        x = self.layer3(x)      # Finalni linearni sloj (bez aktivacije)\n","        return x\n","\n","    def get_info(self):\n","        \"\"\"Vrati informacije o mreži.\"\"\"\n","        total_params = sum(p.numel() for p in self.parameters())\n","        return f\"CustomNetwork: {self.architecture}, Parameters: {total_params}\"\n","\n","# Izgradi našu prilagođenu mrežu\n","custom_network = CustomNetwork()\n","print(\"=== Custom Neural Network Class ===\")\n","print(custom_network.get_info())\n","print(f\"Network architecture:\\n{custom_network}\")"],"metadata":{"id":"MXmvcIKUVl6j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Poređenje Prilagođene Mreže i `nn.Sequential` Mreže"],"metadata":{"id":"3Iw_tIe-WDny"}},{"cell_type":"code","source":["# Poredi sa nn.Sequential verzijom\n","sequential_network = nn.Sequential(\n","    nn.Linear(4, 8),\n","    nn.ReLU(),\n","    nn.Linear(8, 4),\n","    nn.ReLU(),\n","    nn.Linear(4, 1)\n",")\n","\n","print(\"\\nSequential network architecture:\")\n","print(sequential_network)\n","\n","# Kopiraj težine da mreže budu identične za poređenje\n","custom_layers = [custom_network.layer1, custom_network.layer2, custom_network.layer3]\n","sequential_layers = [sequential_network[0], sequential_network[2], sequential_network[4]]\n","\n","for custom_layer, seq_layer in zip(custom_layers, sequential_layers):\n","    custom_layer.weight.data = seq_layer.weight.data.clone()\n","    custom_layer.bias.data = seq_layer.bias.data.clone()\n","\n","# Testiraj obe mreže sa istim ulazom\n","test_input = torch.randn(5, 4)\n","custom_output = custom_network(test_input)\n","sequential_output = sequential_network(test_input)\n","\n","print(f\"\\nTesting both approaches:\")\n","print(f\"Input shape: {test_input.shape}\")\n","print(f\"Custom network output shape: {custom_output.shape}\")\n","print(f\"Sequential network output shape: {sequential_output.shape}\")\n","print(f\"Outputs are identical: {torch.allclose(custom_output, sequential_output)}\")\n","print(f\"Max difference: {torch.abs(custom_output - sequential_output).max():.10f}\")\n","\n","# Poređenje parametara\n","custom_params = sum(p.numel() for p in custom_network.parameters())\n","sequential_params = sum(p.numel() for p in sequential_network.parameters())\n","\n","print(f\"\\nParameter count:\")\n","print(f\"Custom network: {custom_params}\")\n","print(f\"Sequential network: {sequential_params}\")\n","print(f\"Same parameter count: {custom_params == sequential_params}\")"],"metadata":{"id":"uXbL4eVgWKfI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"tiixQeX0UGP-"}},{"cell_type":"markdown","source":["# **Deo 4: PyTorch Optimizatori**"],"metadata":{"id":"LzGHLv2BqHMW"}},{"cell_type":"markdown","source":["## 4.1 Šta su Optimizatori?\n","\n","U Delu 3, naučili smo kako da gradimo neuronske mreže koje mogu da prave predviđanja. Ali kako one zapravo **uče** iz podataka da naprave **tačna predviđanja**? E, tu dolaze optimizatori!\n","\n","**Optimizator je kao trener** koji govori mreži kako da poboljša svoje performanse. Posmatrajte ovo na sledeći način:\n","- Vaša mreža pravi predviđanja (neka dobra, neka loša)\n","- Funkcija gubitka meri koliko su predviđanja pogrešna\n","- Optimizator koristi ovo merenje da shvati kako da prilagodi parametre mreže da pravi bolja predviđanja\n","\n","U analogiji sa fizikom, zamislite da pokušavate da nađete najnižu tačku na potencijalnoj površi (minimum gubitka). Optimizator je vaša strategija za efikasno hodanje nizbrdo. Najčešće je to neka vrsta **gradijentnog spusta**."],"metadata":{"id":"yR0W39n4qZEF"}},{"cell_type":"code","source":["# Učitajmo PyTorch ponovo, ali ovaj put se uverimo da uključimo torch.optim kao optim\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Postavi random seed za reproducibilnost\n","torch.manual_seed(42)\n","print(\"PyTorch version:\", torch.__version__)"],"metadata":{"id":"uMRRhmCiqmeX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Razumevanje Gradijentnog Spusta"],"metadata":{"id":"y18tIdq7q11o"}},{"cell_type":"code","source":["# Definiši našu jednostavnu 1D funkciju gubitka\n","def simple_loss_function(x):\n","    \"\"\"Jednostavna kvadratna funkcija gubitka: f(x) = (x - 2)^2 + 1\"\"\"\n","    return (x - 2) ** 2 + 1\n","\n","def gradient_of_loss(x):\n","    \"\"\"Gradijent (izvod) naše funkcije gubitka: f'(x) = 2(x - 2)\"\"\"\n","    return 2 * (x - 2)\n","\n","# Kreiraj tačke podataka za vizualizaciju\n","x_range = torch.linspace(-1, 5, 100)\n","loss_values = simple_loss_function(x_range)\n","\n","# Početna tačka za naš \"parametar\"\n","x_current = torch.tensor(4.0, requires_grad=True)\n","\n","# Kreiraj početnu vizualizaciju\n","plt.figure(figsize=(15, 5))\n","\n","# Plot 1: Predeo gubitka (Loss landscape)\n","plt.subplot(1, 3, 1)\n","plt.plot(x_range.numpy(), loss_values.numpy(), 'b-', linewidth=2, label='Loss Function')\n","plt.axvline(x=2.0, color='r', linestyle='--', alpha=0.7, label='True Minimum (x=2)')\n","\n","# Koristi .item() da dobiješ skalarnu vrednost za crtanje\n","plt.scatter([x_current.item()], [simple_loss_function(x_current).item()],\n","           color='orange', s=100, label=f'Starting Point (x={x_current.item():.1f})')\n","plt.xlabel('Parameter Value (x)')\n","plt.ylabel('Loss Value')\n","plt.title('Loss Landscape')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","# Sada hajde da izvršimo gradijentni spust\n","learning_rate = 0.1\n","num_steps = 10\n","history = []\n","\n","# Čuvaj početno stanje\n","with torch.no_grad():  # Koristi no_grad da izbegneš građenje computation graph-a\n","    history.append((x_current.item(), simple_loss_function(x_current).item()))\n","\n","print(\"Gradient Descent Steps:\")\n","print(f\"Step 0: x = {x_current.item():.4f}, Loss = {simple_loss_function(x_current).item():.4f}\")\n","\n","for step in range(num_steps):\n","    # Izračunaj gubitak\n","    loss = simple_loss_function(x_current)\n","\n","    # Izračunaj gradijent\n","    loss.backward()  # Ovo računa gradijent\n","\n","    # Ažuriraj parametar koristeći gradijentni spust\n","    with torch.no_grad():\n","        x_current -= learning_rate * x_current.grad\n","        # Čuvaj istoriju za crtanje\n","        history.append((x_current.item(), simple_loss_function(x_current).item()))\n","        print(f\"Step {step+1}: x = {x_current.item():.4f}, Loss = {simple_loss_function(x_current).item():.4f}, Gradient = {x_current.grad.item():.4f}\")\n","\n","    # Postavi gradijent na nulu za sledeću iteraciju\n","    x_current.grad.zero_()\n","\n","# Plot 2: Progres optimizacije\n","plt.subplot(1, 3, 2)\n","steps = list(range(len(history)))\n","losses = [h[1] for h in history]\n","plt.plot(steps, losses, 'go-', linewidth=2, markersize=6)\n","plt.xlabel('Optimization Step')\n","plt.ylabel('Loss Value')\n","plt.title('Loss During Optimization')\n","plt.grid(True, alpha=0.3)\n","\n","# Plot 3: Putanja parametra na predelu gubitka\n","plt.subplot(1, 3, 3)\n","plt.plot(x_range.numpy(), loss_values.numpy(), 'b-', linewidth=2, alpha=0.7, label='Loss Function')\n","plt.axvline(x=2.0, color='r', linestyle='--', alpha=0.7, label='True Minimum')\n","\n","# Nacrtaj putanju koju je prošao gradijentni spust\n","x_positions = [h[0] for h in history]\n","y_positions = [h[1] for h in history]\n","plt.plot(x_positions, y_positions, 'go-', linewidth=2, markersize=6, alpha=0.8, label='GD Path')\n","plt.scatter([x_positions[0]], [y_positions[0]], color='orange', s=100, label='Start', zorder=5)\n","plt.scatter([x_positions[-1]], [y_positions[-1]], color='red', s=100, label='End', zorder=5)\n","\n","plt.xlabel('Parameter Value (x)')\n","plt.ylabel('Loss Value')\n","plt.title('Gradient Descent Path')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Rezime\n","print(f\"\\nSummary:\")\n","print(f\"Started at x = {history[0][0]:.4f} with loss = {history[0][1]:.4f}\")\n","print(f\"Ended at x = {history[-1][0]:.4f} with loss = {history[-1][1]:.4f}\")\n","print(f\"True minimum is at x = 2.0000 with loss = 1.0000\")\n","print(f\"Error from true minimum: {abs(history[-1][0] - 2.0):.4f}\")"],"metadata":{"id":"Mgnyfd6vq2Uz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Vizuelizacija Efekata Stope Učenja"],"metadata":{"id":"tHsO800cq4Wz"}},{"cell_type":"code","source":["# Simuliraj gradijentni spust sa različitim stopama učenja\n","def gradient_descent_simulation(learning_rate, num_steps=15):\n","    \"\"\"Simuliraj gradijentni spust na našoj jednostavnoj funkciji\"\"\"\n","    x = 4  # Početna tačka\n","    history = [x]\n","    loss_history = [simple_loss_function(torch.tensor(x)).item()]\n","\n","    for step in range(num_steps):\n","        gradient = gradient_of_loss(torch.tensor(x))\n","        x = x - learning_rate * gradient.item()\n","        history.append(x)\n","        loss_history.append(simple_loss_function(torch.tensor(x)).item())\n","\n","    return history, loss_history\n","\n","# Testiraj različite stope učenja\n","learning_rates = [0.1, 0.3, 1.0, 1.03]\n","colors = ['blue', 'green', 'red', 'purple']\n","\n","plt.figure(figsize=(14, 5))\n","\n","plt.subplot(1, 2, 1)\n","# Nacrtaj funkciju gubitka\n","x_range = torch.linspace(-1, 5, 100)\n","loss_values = simple_loss_function(x_range)\n","plt.plot(x_range.numpy(), loss_values.numpy(), 'k-', linewidth=2, alpha=0.3, label='Loss Function')\n","\n","for lr, color in zip(learning_rates, colors):\n","    x_hist, loss_hist = gradient_descent_simulation(lr)\n","    # Nacrtaj putanju na funkciji gubitka\n","    x_points = torch.tensor(x_hist)\n","    y_points = simple_loss_function(x_points)\n","    plt.plot(x_points.numpy(), y_points.numpy(), 'o-', color=color, alpha=0.7,\n","            markersize=4, label=f'LR = {lr}')\n","\n","plt.axvline(x=2.0, color='r', linestyle='--', alpha=0.5, label='True Minimum')\n","plt.xlabel('Parameter Value')\n","plt.ylabel('Loss')\n","plt.title('Gradient Descent Paths')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","plt.subplot(1, 2, 2)\n","for lr, color in zip(learning_rates, colors):\n","    x_hist, loss_hist = gradient_descent_simulation(lr)\n","    plt.plot(loss_hist, 'o-', color=color, alpha=0.7, markersize=4, label=f'LR = {lr}')\n","\n","plt.xlabel('Optimization Step')\n","plt.ylabel('Loss Value')\n","plt.title('Loss vs Training Steps')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"Efekti različitih stopa učenja:\")\n","print(\"🔵 LR = 0.1: Spor ali stabilan napredak\")\n","print(\"🟢 LR = 0.5: Dobra ravnoteža brzine i stabilnosti\")\n","print(\"🔴 LR = 1.0: Na granici stabilnosti\")\n","print(\"🟣 LR = 1.03: Previsoka! Oscilacije i nestabilnost\")"],"metadata":{"id":"jKFgCAdBq6XC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"ulu4nrMHrAQj"}},{"cell_type":"markdown","source":["## 4.2 Najčešće Korišćeni Optimizatori\n","\n","Da biste pristupili ugrađenim PyTorch optimizatorima, ne zaboravite da uradite `import torch.optim as optim`. Ako želite, možete takođe napraviti prilagođeni optimizator, ali to nećemo raditi na ovom kursu.\n","\n","Pogledajmo najpopularnije optimizatore u PyTorch-u:\n","*   SGD (Stochastic Gradient Descent)\n","*   Adam (Adaptive Moment Estimation) - Veoma popularan!\n","*   RMSprop"],"metadata":{"id":"f-eKbP4crBR7"}},{"cell_type":"code","source":["# Kreiraj jednostavan model za demonstraciju optimizatora\n","class SimpleModel(nn.Module):\n","    def __init__(self):\n","        super(SimpleModel, self).__init__()\n","        self.linear = nn.Linear(1, 1)\n","\n","    def forward(self, x):\n","        return self.linear(x)\n","\n","# Kreiraj model i neke lažne podatke\n","model = SimpleModel()\n","x_data = torch.randn(10, 1)\n","y_data = torch.randn(10, 1)\n","\n","print(\"=== Common Optimizers ===\")\n","\n","# 1. SGD (Stochastic Gradient Descent)\n","sgd_optimizer = optim.SGD(model.parameters(), lr=0.01)\n","print(f\"SGD: {sgd_optimizer}\")\n","\n","# 2. Adam (Adaptive Moment Estimation)\n","adam_optimizer = optim.Adam(model.parameters(), lr=0.01)\n","print(f\"Adam: {adam_optimizer}\")\n","\n","# 3. RMSprop\n","rmsprop_optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n","print(f\"RMSprop: {rmsprop_optimizer}\")\n","\n","print(f\"\\nKey parameters:\")\n","print(f\"- lr (learning rate): Koliko veliki koraci da se prave\")\n","print(f\"- Većina optimizatora ima dodatne parametre za fino podešavanje\")"],"metadata":{"id":"TzASvR6BrGAY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"r4wM9ahOWPGu"}},{"cell_type":"markdown","source":["# Ključni Zaključci"],"metadata":{"id":"Cf-it3zYWQk7"}},{"cell_type":"markdown","source":["## Šta smo naučili:\n","\n","1. **Univerzalni obrazac**: Gradivni blokovi u PyTorch-u (slojevi, aktivacije, gubici) su podklase osnovne `nn.Module` klase\n","2. **Ugrađeni moduli**: Već napravljene komponente koje prate konzistentne interfejse\n","3. **Prilagođeni moduli**: Kako da izgradite svoje komponente od nule\n","4. **Kompozicija modula**: Kako da kombinujete module u veće sisteme\n","5. **Ispod haube**: Šta se stvarno dešava unutar čestih PyTorch komponenti\n","6. **Optimizatori**: Algoritmi koji čine da neuronske mreže uče"],"metadata":{"id":"97kVr3MUWVU-"}},{"cell_type":"markdown","source":["## PyTorch filozofija:\n","\n","- **Modularnost**: Mali, iznova upotrebljivi delovi\n","- **Konzistentnost**: Isti interfejs za sve komponente\n","- **Transparentnost**: Lako je videti i menjati šta se dešava unutra\n","- **Kompozibilnost**: Jednostavni delovi se kombinuju da naprave složene sisteme"],"metadata":{"id":"4r39EglRXqdl"}},{"cell_type":"markdown","source":["## Sledeći koraci:\n","\n","U sledećem odeljku, naučićemo o **Radnim tokovima u PyTorch-u** - kako da stvarno sve spojimo u jednu celinu. Koristićemo module koje sada razumemo i dodati finalne delove potrebne za mašinsko učenje."],"metadata":{"id":"rp0KouMMXsHU"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"PR046h-4Wl-6"}},{"cell_type":"markdown","source":["# Samostalni Rad\n","\n","Zapamtite, svaki prilagođeni modul treba da ima:\n","1. super().**__init__**() u **init**\n","2. nn.Parameter() ako ima parametre koji se mogu učiti\n","3. forward() metodu za izračunavanje"],"metadata":{"id":"-IpRZ1zTWp87"}},{"cell_type":"markdown","source":["## Zadaci\n","\n","1. Kreirajte prilagođenu Sigmoid aktivaciju\n","    - Savet: sigmoid(x) = 1 / (1 + exp(-x))\n","    - Koristite torch.exp() i pazite na numeričku stabilnost\n","2. Kreirajte prilagođenu L1Loss (Mean Absolute Error) funkciju gubitka\n","    - Savet: L1(prediction, target) = mean(|prediction - target|)\n","    - Koristite torch.abs()\n","3. Kreirajte prilagođeni sloj koji dodaje \"učljiv\" pomeraj  **b** bilo kom ulazu **f(x) = x + b**\n","    - Ovaj sloj treba da doda različit pomeraj svakoj karakteristici ulaznih podataka\n","4. Kombinujte svoje prilagođene module\n","    - Izgradite mrežu koristeći vašu prilagođenu Sigmoid i L1Loss\n","    - Izgradite je sa PyTorch-ovim ugrađenim ekvivalentima"],"metadata":{"id":"wQKd-RRmWr3J"}},{"cell_type":"code","source":["# Mesto za rešavanje Zadatka 1"],"metadata":{"id":"Fe8lv3RdXhwm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mesto za rešavanje Zadatka 2"],"metadata":{"id":"wJ_guypzXho1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mesto za rešavanje Zadatka 3"],"metadata":{"id":"zN_sHwW-Xhhq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mesto za rešavanje Zadatka 4"],"metadata":{"id":"Q9ySmxY3XhR1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"wrZ2jsWExUK4"}},{"cell_type":"markdown","source":["## Rešenja"],"metadata":{"id":"Fp9BL0awxU4-"}},{"cell_type":"code","source":["# -------- Zadatak 1 --------\n","import torch\n","import torch.nn as nn\n","\n","class CustomSigmoid(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        # Numerički stabilna sigmoid funkcija:\n","        # Za x >= 0: sigmoid(x) = 1 / (1 + exp(-x))\n","        # Za x < 0 : sigmoid(x) = exp(x) / (1 + exp(x))\n","        out = torch.empty_like(x)\n","        pos_mask = (x >= 0)\n","        neg_mask = ~pos_mask\n","\n","        # Stabilno izračunavanje za pozitivne vrednosti\n","        out[pos_mask] = 1 / (1 + torch.exp(-x[pos_mask]))\n","\n","        # Stabilno izračunavanje za negativne vrednosti\n","        exp_x = torch.exp(x[neg_mask])\n","        out[neg_mask] = exp_x / (1 + exp_x)\n","\n","        return out"],"metadata":{"id":"ZpDhNRQjxXG1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------- Zadatak 2 --------\n","import torch\n","import torch.nn as nn\n","\n","class CustomL1Loss(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, pred, target):\n","        return torch.mean(torch.abs(pred - target))\n"],"metadata":{"id":"P3gCKtUEx200"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------- Zadatak 3 --------\n","import torch\n","import torch.nn as nn\n","\n","class BiasLayer(nn.Module):\n","    \"\"\"\n","    Dodaje učljiv pomeraj svakoj karakteristici: f(x) = x + b\n","    Radi za ulaze oblika (batch_size, num_features).\n","    \"\"\"\n","    def __init__(self, num_features):\n","        super().__init__()\n","        # bias (pomeraj) je učljiv parametar oblika (num_features,)\n","        self.bias = nn.Parameter(torch.zeros(num_features))\n","\n","    def forward(self, x):\n","        # Broadcasting automatski dodaje bias svakom uzorku u batch-u\n","        return x + self.bias"],"metadata":{"id":"-87wLKuax9zn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------- Zadatak 4 --------\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","torch.manual_seed(0)\n","N, D_in, D_out = 512, 10, 1\n","\n","# ---- Dva modela: (A) prilagođen, (B) ugrađen ----\n","class TinyNetCustom(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.lin = nn.Linear(D_in, D_out, bias=False)\n","        self.bias = BiasLayer(D_out)        # učljiv bias\n","        self.act = CustomSigmoid()          # prilagođena sigmoid funkcija\n","\n","    def forward(self, x):\n","        return self.act(self.bias(self.lin(x)))\n","\n","class TinyNetBuiltin(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.lin = nn.Linear(D_in, D_out, bias=False)\n","        self.bias = BiasLayer(D_out)\n","\n","    def forward(self, x):\n","        return torch.sigmoid(self.bias(self.lin(x)))\n","\n","netA = TinyNetCustom()\n","netB = TinyNetBuiltin()\n","\n","# Inicijalizuj obe mreže sa istim težinama za ravnopravno poređenje\n","with torch.no_grad():\n","    netB.lin.weight.copy_(netA.lin.weight)\n","    netB.bias.bias.copy_(netA.bias.bias)\n","\n","lossA = CustomL1Loss()\n","lossB = nn.L1Loss()\n","\n","optA = optim.SGD(netA.parameters(), lr=0.5)\n","optB = optim.SGD(netB.parameters(), lr=0.5)"],"metadata":{"id":"dppyqJn5yNgM"},"execution_count":null,"outputs":[]}]}