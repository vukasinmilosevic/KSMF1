{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPqynHVpo75eKK4AafL4iYP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **PyTorch Osnove 2: Razumevanje Modularnog Sistema PyTorch-a**\n","\n","**Kurs:** KSMF1  \n","**Trajanje:** ~45 min  \n","**Preduslovi:** Osnove PyTorch tenzora (PyTorch Osnove 1)\n","\n","---"],"metadata":{"id":"_tUUiYuVQTwh"}},{"cell_type":"markdown","source":["## Ciljevi Sekcije\n","\n","Do kraja ove sekcije, nauƒçiƒáete da:\n","\n","- Razumete da je sve u PyTorch-u (slojevi, aktivacije, gubici) `nn.Module`\n","- Koristite **ugraƒëene** module kao ≈°to su `nn.Linear`, `nn.ReLU`, i `nn.MSELoss`\n","  - Izgradite jednostavne neuronske mre≈æe kombinovanjem **ugraƒëenih** modula\n","- Kreirate **prilagoƒëene** module od nule\n","  - Izgradite jednostavne neuronske mre≈æe kombinovanjem **prilagoƒëenih** modula\n","- Razumete konzistentan obrazac koji sve PyTorch komponente prate\n","---"],"metadata":{"id":"F07O87i4Qf-x"}},{"cell_type":"markdown","source":["# **Deo 3: Uvod u PyTorch Module**"],"metadata":{"id":"393udRm1Qk7d"}},{"cell_type":"markdown","source":["## 3.1 ≈†ta je Modul?\n","\n","U PyTorch-u, **modul** je gradivni blok neuronske mre≈æe (modela).\n","\n","Posmatrajte PyTorch module kao **LEGO kockice** za gradnju neuronskih mre≈æa. Ba≈° kao ≈°to mo≈æete da kombinujete jednostavne LEGO delove da biste izgradili slo≈æene strukture, mo≈æete kombinovati jednostavne komponente neuronskih mre≈æa da biste izgradili sofisticirane modele.\n","\n","Ti gradivni blokovi su najƒçe≈°ƒáe \"koraci obrade\" koji uzimaju neki ulaz x i vraƒáaju izlaz f(x). U ovom kontekstu, mo≈æemo videti da je \"potpuno povezani sloj\" modul, aktivaciona funkcija je takoƒëe modul, ƒçak su i funkcije gubitka moduli!"],"metadata":{"id":"OHFT3IinQx4V"}},{"cell_type":"code","source":["# Uƒçitajmo PyTorch i druge va≈æne biblioteke\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Postavi random seed za reproducibilnost\n","torch.manual_seed(42)\n","print(\"PyTorch version:\", torch.__version__)"],"metadata":{"id":"uDbabd4YQ4lE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"KVdEXUpnRKWH"}},{"cell_type":"markdown","source":["## 3.2 Osnovna klasa: **`nn.Module`**\n","\n","Svaka komponenta neuronske mre≈æe u PyTorch-u je podklasa (nasleƒëuje od) `nn.Module`. Ne brinite ako niste upoznati sa klasama - posmatrajte `nn.Module` kao **≈°ablon** koji pru≈æa zajedniƒçku funkcionalnost za sve gradivne blokove neuronskih mre≈æa.\n","\n","U PyTorch-u, mo≈æete definisati svoje **prilagoƒëene module**, ali on takoƒëe dolazi sa velikim brojem veƒá **ugraƒëenih modula** za lako i brzo implementiranje. Svi oni su **podklase** od `nn.Module`."],"metadata":{"id":"tHkJUc8yRLdg"}},{"cell_type":"markdown","source":["### ≈†ta `nn.Module` Pru≈æa"],"metadata":{"id":"Bp2ad0sIRWRN"}},{"cell_type":"code","source":["# Hajde da pogledamo ≈°ta nn.Module pru≈æa\n","print(\"Methods available in nn.Module:\")\n","module_methods = [method for method in dir(nn.Module) if not method.startswith('_')]\n","print(f\"Total methods: {len(module_methods)}\")\n","print(\"List of methods:\", module_methods)"],"metadata":{"id":"MHr-3ilhRfNS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Najva≈ænije Metode"],"metadata":{"id":"XdGb_jHLRiWG"}},{"cell_type":"markdown","source":["Kljuƒçne metode koje ƒáemo koristiti:\n","\n","  - **forward():** Defini≈°e kako podaci prolaze kroz modul (defini≈°e **f(x; w)**)\n","  - **parameters():** Vraƒáa sve parametre **w** koji se mogu trenirati\n","  - **train():** Postavlja modul u re≈æim treniranja\n","  - **eval()**: Postavlja modul u re≈æim evaluacije"],"metadata":{"id":"vFDLVHEy1tnT"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"eTc1A9bwRxzx"}},{"cell_type":"markdown","source":["## 3.3 Ugraƒëeni (Built-in) Moduli\n","\n","PyTorch dolazi sa velikim brojem veƒá definisanih modula za gradnju neuronskih mre≈æa. Nazivamo ih **ugraƒëeni slojevi** i oni ukljuƒçuju skoro sve vrste slojeva za obradu, aktivacionih slojeva i funkcija gubitka koje se ƒçesto koriste.\n","\n","Pogledajmo neke primere:"],"metadata":{"id":"QFlQzlpgSCDp"}},{"cell_type":"markdown","source":["### Primer 1 - Linearni Slojevi (Potpuno Povezani Slojevi)\n","\n","Linearni sloj izvr≈°ava operaciju: **izlaz = te≈æine √ó ulaz + pomeraj**, gde je ulaz vektor veliƒçine N, te≈æine su MxN matrica, a pomeraj je vektor veliƒçine M. Ovo ƒçini izlazni vektor veliƒçine M.\n","\n","Ovo je bukvalno jednaƒçina prave: **f(x) = wx + b**, ali pro≈°irena na vi≈°e dimenzija (jednaƒçina hiper-ravni)!"],"metadata":{"id":"QsRBd7tNS_Nh"}},{"cell_type":"code","source":["# Kreirajmo linearni sloj koristeƒái ugraƒëenu klasu nn.Linear\n","# Ulazne karakteristike: 3 (npr., x, y, z koordinate)\n","# Izlazne karakteristike: 2 (npr., energija, impuls)\n","\n","linear = nn.Linear(in_features=3, out_features=2)\n","\n","# Ispitaj parametre\n","print(f\"\\nParameters:\")\n","print(f\"Weight shape: {linear.weight.shape}\")  # (out_features, in_features)\n","print(f\"Bias shape: {linear.bias.shape}\")      # (out_features,)\n","print(f\"Weight matrix:\\n{linear.weight.data}\")\n","print(f\"Bias vector: {linear.bias.data}\")\n","\n","# Koristi sloj\n","input_data = torch.randn(4, 3)  # Kreiraj izmi≈°ljeni batch podataka od 4 uzorka, 3 karakteristike svaki\n","output = linear(input_data)  # Ovde koristimo sloj kao funkciju\n","\n","print(f\"\\nUsage:\")\n","print(f\"Input shape: {input_data.shape}\")\n","print(f\"Output shape: {output.shape}\")\n","print(f\"Input:\\n{input_data}\")\n","print(f\"Output:\\n{output}\")\n","\n","# Proveri izraƒçun ruƒçno\n","manual_output = torch.matmul(input_data, linear.weight.t()) + linear.bias\n","print(f\"\\nManual computation matches PyTorch: {torch.allclose(output, manual_output)}\")"],"metadata":{"id":"appIHp5FTAU6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Primer 2 - Aktivacione Funkcije\n","\n","Aktivacione funkcije uvode **nelinearnost** u neuronske mre≈æe. Bez njih, vi≈°estruki linearni slojevi bi bili ekvivalentni jednom linearnom sloju!\n","\n","Ispitajmo neke od najƒçe≈°ƒáih aktivacionih funkcija:"],"metadata":{"id":"6Hb92Zx8TLxH"}},{"cell_type":"code","source":["# Pripremi x-osu za vizualizaciju\n","x = torch.linspace(-3, 3, 100)\n","\n","# ReLU (Rectified Linear Unit): f(x) = max(0, x)\n","relu = nn.ReLU()\n","relu_output = relu(x)\n","\n","# Sigmoid: f(x) = 1 / (1 + e^(-x))\n","sigmoid = nn.Sigmoid()\n","sigmoid_output = sigmoid(x)\n","\n","# Tanh: f(x) = tanh(x)\n","tanh = nn.Tanh()\n","tanh_output = tanh(x)\n","\n","# Nacrtaj aktivacione funkcije\n","plt.figure(figsize=(12, 4))\n","\n","plt.subplot(1, 3, 1)\n","plt.plot(x.numpy(), relu_output.numpy(), 'r-', linewidth=2)\n","plt.title('ReLU Activation')\n","plt.grid(True)\n","plt.xlabel('Input')\n","plt.ylabel('Output')\n","\n","plt.subplot(1, 3, 2)\n","plt.plot(x.numpy(), sigmoid_output.numpy(), 'g-', linewidth=2)\n","plt.title('Sigmoid Activation')\n","plt.grid(True)\n","plt.xlabel('Input')\n","plt.ylabel('Output')\n","\n","plt.subplot(1, 3, 3)\n","plt.plot(x.numpy(), tanh_output.numpy(), 'b-', linewidth=2)\n","plt.title('Tanh Activation')\n","plt.grid(True)\n","plt.xlabel('Input')\n","plt.ylabel('Output')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"ReLU: Popularan jer je jednostavan i dobro radi\")\n","print(\"Sigmoid: Izlazi izmeƒëu 0 i 1, dobar za verovatnoƒáe\")\n","print(\"Tanh: Izlazi izmeƒëu -1 i 1, centriran na nuli\")"],"metadata":{"id":"M32ySLA_TOEc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Primer 3 - Funkcije Gubitka\n","\n","Funkcije gubitka mere koliko su predviƒëanja modela pogre≈°na tako ≈°to uzimaju na≈° finalni izlaz i poznati rezultat iz dataset-a kao svoje ulaze, i primenjuju neku metriku da izmere koliko su razliƒçiti.\n","\n","**Funkcije gubitka u PyTorch-u su takoƒëe nn.Module-i!** To znaƒçi da prate iste obrasce koje smo uƒçili."],"metadata":{"id":"LE9qfsqmTT2C"}},{"cell_type":"code","source":["# Pogledajmo razliƒçite tipove funkcija gubitka\n","print(\"\\n=== Common Loss Functions ===\")\n","\n","# Za regresiju (predviƒëanje kontinuiranih vrednosti)\n","print(\"Regression losses:\")\n","print(\"- nn.MSELoss: Mean Squared Error\")\n","print(\"- nn.L1Loss: Mean Absolute Error\")\n","print(\"- nn.SmoothL1Loss: Huber loss (otporan na izuzetke)\")\n","\n","# Za klasifikaciju (predviƒëanje kategorija)\n","print(\"\\nClassification losses:\")\n","print(\"- nn.CrossEntropyLoss: Za multiklasnu klasifikaciju\")\n","print(\"- nn.BCELoss: Binary Cross Entropy za binarnu klasifikaciju\")\n","print(\"- nn.NLLLoss: Negative Log Likelihood\")\n","\n","# Kreiranje instanci\n","mse_loss = nn.MSELoss()\n","cross_entropy_loss = nn.CrossEntropyLoss()"],"metadata":{"id":"KdYLsj0pTVyx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Razumevanje Razliƒçitih Funkcija Gubitka"],"metadata":{"id":"fdD6UXgeTZpl"}},{"cell_type":"code","source":["# Uporedimo razliƒçite funkcije gubitka na primerima\n","print(\"=== Loss Function Comparison ===\")\n","\n","# Primer regresije\n","predicted_energy = torch.tensor([10.2, 15.8, 8.1])\n","actual_energy = torch.tensor([10.0, 16.0, 8.5])\n","\n","mse = nn.MSELoss()(predicted_energy, actual_energy)\n","mae = nn.L1Loss()(predicted_energy, actual_energy)\n","smooth_l1 = nn.SmoothL1Loss()(predicted_energy, actual_energy)\n","\n","print(\"Regression losses:\")\n","print(f\"Predictions: {predicted_energy}\")\n","print(f\"Actual: {actual_energy}\")\n","print(f\"MSE Loss: {mse:.4f}\")\n","print(f\"MAE Loss: {mae:.4f}\")\n","print(f\"Smooth L1 Loss: {smooth_l1:.4f}\")\n","\n","# Primer klasifikacije\n","raw_scores = torch.tensor([\n","    [2.1, 0.5, -1.2],  # Sna≈æno predviƒëa klasu 0\n","    [-0.8, 1.9, 0.1],  # Sna≈æno predviƒëa klasu 1\n","    [0.2, -1.1, 1.8]   # Sna≈æno predviƒëa klasu 2\n","])\n","true_labels = torch.tensor([0, 1, 2])\n","\n","cross_entropy = nn.CrossEntropyLoss()(raw_scores, true_labels)\n","\n","print(f\"\\nClassification loss:\")\n","print(f\"Raw scores:\\n{raw_scores}\")\n","print(f\"True labels: {true_labels}\")\n","print(f\"CrossEntropy Loss: {cross_entropy:.4f}\")"],"metadata":{"id":"yTX_-M1kTcm0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"FagFTRMUT7jB"}},{"cell_type":"markdown","source":["## 3.4 Gradnja Neuronske Mre≈æe sa Ugraƒëenim Modulima\n","\n","Hajde da iskombinujemo ugraƒëene module da izgradimo jednostavnu neuronsku mre≈æu!\n","\n","**VEOMA VA≈ΩNO**: Ba≈° kao ≈°to slojevi uzimaju ulaz x i vraƒáaju izlaz f(x), isto to radi i cela mre≈æa. To znaƒçi da je **cela mre≈æa zapravo `nn.Module`!**\n","\n","Ovo nam omoguƒáava da koristimo \"slojeve apstrakcije\", tj. da koristimo na≈°e mre≈æe kao gradivne blokove za jo≈° slo≈æenije strukture.\n","\n","Postoje dva naƒçina da se iskombinuju moduli u jedan veƒái modul (ili punu mre≈æu):\n","1. Kori≈°ƒáenje `nn.Sequential`: Ovo je modul koji automatski sla≈æe module jedan za drugim. Pojednostavljuje gradnju mre≈æa gde slojevi dolaze u sekvenci.\n","2. Definisanje veƒáeg modula kao **prilagoƒëeni modul**: Ovo je fleksibilniji metod za nesekvencijalno slaganje modula. Nauƒçiƒáemo kako da pravimo prilagoƒëene module u sledeƒáem odeljku."],"metadata":{"id":"XLMygTspT8vz"}},{"cell_type":"markdown","source":["### Kori≈°ƒáenje `nn.Sequential`"],"metadata":{"id":"A4UuWFh-UZqs"}},{"cell_type":"code","source":["# Metod 1: Kori≈°ƒáenje nn.Sequential (automatski sla≈æemo na≈°e slojeve)\n","simple_network = nn.Sequential(\n","    nn.Linear(4, 8),     # 4 ulazne karakteristike ‚Üí 8 skrivenih jedinica\n","    nn.ReLU(),           # Aktivacijska funkcija\n","    nn.Linear(8, 4),     # 8 skrivenih jedinica ‚Üí 4 skrivene jedinice\n","    nn.ReLU(),           # Jo≈° jedna aktivacija\n","    nn.Linear(4, 1)      # 4 skrivene jedinice ‚Üí 1 izlaz\n",")\n","\n","print(\"=== Simple Neural Network ===\")\n","print(\"Network architecture:\")\n","print(simple_network)\n","print(f\"\\nThe network is an nn.Module: {isinstance(simple_network, nn.Module)}\")\n","\n","# Testiraj mre≈æu\n","test_input = torch.randn(3, 4)  # 3 uzorka, 4 karakteristike svaki\n","network_output = simple_network(test_input)\n","\n","print(f\"\\nTesting the network:\")\n","print(f\"Input shape: {test_input.shape}\")\n","print(f\"Output shape: {network_output.shape}\")\n","print(f\"Input:\\n{test_input}\")\n","print(f\"Output:\\n{network_output}\")\n","\n","# Prebroj ukupne parametre\n","total_params = sum(p.numel() for p in simple_network.parameters())\n","print(f\"\\nTotal parameters: {total_params}\")\n","\n","# Detaljni pregled parametara\n","print(\"\\nParameter breakdown:\")\n","for i, layer in enumerate(simple_network):\n","    if hasattr(layer, 'weight'):\n","        weight_params = layer.weight.numel()\n","        bias_params = layer.bias.numel() if layer.bias is not None else 0\n","        total_layer_params = weight_params + bias_params\n","        print(f\"Layer {i} ({layer.__class__.__name__}): {total_layer_params} parameters\")\n","        print(f\"  - Weights: {layer.weight.shape} = {weight_params} params\")\n","        print(f\"  - Bias: {layer.bias.shape} = {bias_params} params\")\n","    else:\n","        print(f\"Layer {i} ({layer.__class__.__name__}): 0 parameters\")"],"metadata":{"id":"5zd8bixvUFnB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Testiranje sa Funkcijom Gubitka"],"metadata":{"id":"BxKOrdkEUdPi"}},{"cell_type":"code","source":["# Hajde da koristimo na≈°u mre≈æu sa funkcijom gubitka\n","print(\"=== Network + Loss Function ===\")\n","\n","# Kreiraj neke la≈æne ciljne podatke za na≈° test\n","test_targets = torch.randn(3, 1)  # 3 uzorka, 1 cilj svaki\n","loss_function = nn.MSELoss()\n","\n","# Izraƒçunaj gubitak\n","loss = loss_function(network_output, test_targets)\n","\n","print(f\"Network predictions:\\n{network_output}\")\n","print(f\"Target values:\\n{test_targets}\")\n","print(f\"MSE Loss: {loss:.4f}\")"],"metadata":{"id":"FMMpJr4hUfaA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"YJeje2UKUiCN"}},{"cell_type":"markdown","source":["## 3.5 Prilagoƒëeni Moduli\n","\n","Prava snaga PyTorch-a le≈æi u **kreiranju va≈°ih sopstvenih prilagoƒëenih modula**. Da bismo razumeli kako ovo stvarno funkcioni≈°e, ponovo ƒáemo kreirati neke od modula koje smo videli u prethodnom odeljku, ali ovog puta od nule.\n","\n","U sledeƒáim primerima, gradiƒáemo `Linear layer`, `ReLU activation layer`, i `MSE loss function` kao potklase `nn.Modul`-a:"],"metadata":{"id":"bFasM7ySUjV1"}},{"cell_type":"markdown","source":["### Primer 1 - Prilagoƒëeni Linearni Slojevi\n","\n","Hajde da kreiramo `nn.Linear` sloj sami. Ovo ƒáe nam pokazati kako ugraƒëeni slojevi rade \"ispod haube\"."],"metadata":{"id":"sgJ1fQb4U0vb"}},{"cell_type":"code","source":["class CustomLinear(nn.Module):\n","    \"\"\"\n","    Na≈°a sopstvena implementacija nn.Linear da bismo razumeli kako radi interno.\n","    Ovo je kao pravljenje sopstvenog voltmetra umesto kupovine veƒá napravljenog -\n","    razumete taƒçno kako radi!\n","    \"\"\"\n","    def __init__(self, in_features, out_features, bias=True):\n","        super(CustomLinear, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","\n","        # Kreiraj matricu te≈æina - ovo je ono ≈°to ƒáe mre≈æa uƒçiti!\n","        # BITNO: Moramo da je defini≈°emo kao nn.Parameter da bi PyTorch znao da su to vrednosti koje treba da a≈æurira pri treningu\n","        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n","\n","        if bias:\n","            self.bias = nn.Parameter(torch.randn(out_features))\n","        else:\n","            self.bias = None\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass: izraƒçunaj output = input @ weight.T + bias\n","        Ovo je isto kao: output = weight @ input + bias za svaki uzorak\n","        \"\"\"\n","        # Matriƒçno mno≈æenje: (batch_size, in_features) @ (in_features, out_features)\n","        output = torch.matmul(x, self.weight.t())  # .t() znaƒçi transpozicija\n","\n","        if self.bias is not None:\n","            output = output + self.bias\n","\n","        return output\n","\n","# Poredimo na≈° prilagoƒëeni sloj sa PyTorch-ovim ugraƒëenim slojem\n","\n","# Fiksiraj seed zbog reprudicibilnosti\n","torch.manual_seed(42)\n","\n","# Kreiraj oba sloja sa istim dimenzijama\n","builtin_layer = nn.Linear(3, 2)\n","custom_layer = CustomLinear(3, 2)\n","\n","# Kopiraj te≈æine da budu identiƒçne za poreƒëenje\n","custom_layer.weight.data = builtin_layer.weight.data.clone()\n","custom_layer.bias.data = builtin_layer.bias.data.clone()\n","\n","# Testiraj na istim ulaznim podacima\n","test_input = torch.randn(4, 3)\n","builtin_output = builtin_layer(test_input)\n","custom_output = custom_layer(test_input)\n","\n","print(\"Comparing built-in vs custom linear layer:\")\n","print(f\"Built-in output:\\n{builtin_output}\")\n","print(f\"Custom output:\\n{custom_output}\")\n","print(f\"Maximum difference: {torch.abs(builtin_output - custom_output).max():.10f}\")\n","print(\"They're identical! Our custom layer works perfectly.\")"],"metadata":{"id":"D1QzK0yLU23G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Primer 2 - Prilagoƒëeni ReLU aktivacijski sloj\n","\n","Ba≈° kao i pre, hajde da izgradimo ReLU Aktivacioni Sloj od nule:"],"metadata":{"id":"vevCN0i0VD58"}},{"cell_type":"code","source":["class CustomReLU(nn.Module):\n","    \"\"\"\n","    Na≈°a sopstvena implementacija nn.ReLU.\n","    ReLU(x) = max(0, x)\n","    \"\"\"\n","    def __init__(self):\n","        super(CustomReLU, self).__init__()\n","        # Aktivacione funkcije nemaju parametre za uƒçenje!\n","\n","    def forward(self, x):\n","        \"\"\"Primeni ReLU aktivaciju: f(x) = max(0, x)\"\"\"\n","        return torch.clamp(x, min=0.0)  # Ekvivalentno sa torch.max(x, torch.zeros_like(x))\n","\n","# Testiraj na≈° prilagoƒëeni ReLU\n","print(\"=== Custom ReLU Activation ===\")\n","custom_relu = CustomReLU()\n","builtin_relu = nn.ReLU()\n","\n","print(f\"Custom ReLU: {custom_relu}\")\n","print(f\"Built-in ReLU: {builtin_relu}\")\n","\n","# Testiraj sa pozitivnim i negativnim vrednostima\n","test_values = torch.tensor([-3.0, -1.0, 0.0, 1.0, 3.0])\n","custom_relu_output = custom_relu(test_values)\n","builtin_relu_output = builtin_relu(test_values)\n","\n","print(f\"\\nTesting ReLU:\")\n","print(f\"Input: {test_values}\")\n","print(f\"Custom ReLU: {custom_relu_output}\")\n","print(f\"Built-in ReLU: {builtin_relu_output}\")\n","print(f\"Outputs are identical: {torch.allclose(custom_relu_output, builtin_relu_output)}\")\n","\n","# Proveri da nema parametre\n","print(f\"\\nCustom ReLU parameters: {len(list(custom_relu.parameters()))}\")\n","print(f\"Built-in ReLU parameters: {len(list(builtin_relu.parameters()))}\")\n","print(f\"Both have 0 parameters (activations don't learn!)\")\n","\n","# Vizuelno poreƒëenje\n","x = torch.linspace(-3, 3, 100)\n","custom_y = custom_relu(x)\n","builtin_y = builtin_relu(x)\n","\n","plt.figure(figsize=(8, 5))\n","plt.plot(x.numpy(), custom_y.numpy(), 'r-', linewidth=3, label='Custom ReLU', alpha=0.7)\n","plt.plot(x.numpy(), builtin_y.numpy(), 'b--', linewidth=2, label='Built-in ReLU')\n","plt.xlabel('Input')\n","plt.ylabel('Output')\n","plt.title('Custom vs Built-in ReLU')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","print(\"The curves are identical - our custom ReLU works perfectly!\")"],"metadata":{"id":"vhUhAIBkVEYU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Primer 3 - Prilagoƒëena MSE Funkcija Gubitka\n","\n","Kreirajmo na≈°u sopstvenu MSE funkciju gubitka da bismo razumeli kako funkcije gubitka rade interno:"],"metadata":{"id":"n4qXgwf2VItg"}},{"cell_type":"code","source":["class CustomMSELoss(nn.Module):\n","    \"\"\"\n","    Na≈°a sopstvena implementacija Mean Squared Error gubitka.\n","    MSE = mean((predicted - actual)¬≤)\n","    \"\"\"\n","    def __init__(self, reduction='mean'):\n","        super(CustomMSELoss, self).__init__()\n","        self.reduction = reduction\n","\n","    def forward(self, predicted, actual):\n","        \"\"\"Izraƒçunaj MSE gubitak.\"\"\"\n","        # Izraƒçunaj kvadratne razlike\n","        squared_diff = (predicted - actual) ** 2\n","\n","        # Primeni redukciju\n","        if self.reduction == 'mean':\n","            loss = torch.mean(squared_diff)\n","        elif self.reduction == 'sum':\n","            loss = torch.sum(squared_diff)\n","        elif self.reduction == 'none':\n","            loss = squared_diff\n","        else:\n","            raise ValueError(f\"Invalid reduction: {self.reduction}\")\n","\n","        return loss\n","\n","# Testiraj na≈° prilagoƒëeni MSE\n","custom_mse = CustomMSELoss()\n","builtin_mse = nn.MSELoss()\n","\n","# Test podaci\n","pred = torch.tensor([1.2, 2.8, 3.1])\n","actual = torch.tensor([1.0, 3.0, 3.5])\n","\n","custom_loss = custom_mse(pred, actual)\n","builtin_loss = builtin_mse(pred, actual)\n","\n","print(\"Comparing custom vs built-in MSE loss:\")\n","print(f\"Custom MSE: {custom_loss:.6f}\")\n","print(f\"Built-in MSE: {builtin_loss:.6f}\")\n","print(f\"Difference: {torch.abs(custom_loss - builtin_loss):.10f}\")\n","\n","# Ruƒçno raƒçunanje za verifikaciju\n","manual_mse = torch.mean((pred - actual) ** 2)\n","print(f\"Manual calculation: {manual_mse:.6f}\")"],"metadata":{"id":"j6XG0RdRVK3n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"LVIOiITNVNQc"}},{"cell_type":"markdown","source":["## 3.6 Gradnja Neuronske Mre≈æe sa Prilagoƒëenim Modulima\n","\n","Hajde sada da ponovo izgradimo na≈°u neuronsku mre≈æu iz odeljka 3.4 koristeƒái samo prilagoƒëene module koje smo napravili.\n","\n","Ovaj put, umesto kori≈°ƒáenja nn.Sequential da brzo kombinujemo slojeve, kreiraƒçemo na≈°u sopstvenu klasu prilagoƒëene mre≈æe (zapamtite da su mre≈æe takoƒëe moduli, samo slo≈æeniji)."],"metadata":{"id":"HzOH3dcFVN-w"}},{"cell_type":"markdown","source":["### Neuronska Mre≈æa kao Prilagoƒëeni `nn.Module`"],"metadata":{"id":"Jn_XjkQwVnaB"}},{"cell_type":"code","source":["class CustomNetwork(nn.Module):\n","    \"\"\"\n","    Prilagoƒëena neuronska mre≈æa izgraƒëena u potpunosti od na≈°ih prilagoƒëenih modula.\n","    Ovo pokazuje kako se kreiraju slo≈æene mre≈æe kombinovanjem jednostavnijih modula.\n","    \"\"\"\n","    def __init__(self, input_size=4, hidden_size1=8, hidden_size2=4, output_size=1):\n","        super(CustomNetwork, self).__init__()\n","\n","        # Defini≈°i sve na≈°e slojeve koristeƒái na≈°e prilagoƒëene module\n","        self.layer1 = CustomLinear(input_size, hidden_size1)\n","        self.activation1 = CustomReLU()\n","        self.layer2 = CustomLinear(hidden_size1, hidden_size2)\n","        self.activation2 = CustomReLU()\n","        self.layer3 = CustomLinear(hidden_size2, output_size)\n","\n","        # ƒåuvaj informacije o arhitekturi\n","        self.architecture = f\"{input_size}‚Üí{hidden_size1}‚Üí{hidden_size2}‚Üí{output_size}\"\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Defini≈°i kako podaci prolaze kroz mre≈æu.\n","        Ovde specificiramo taƒçnu putanju izraƒçunavanja.\n","        \"\"\"\n","        # Forward pass kroz svaki sloj\n","        x = self.layer1(x)      # Linearna transformacija\n","        x = self.activation1(x)  # Primeni ReLU\n","        x = self.layer2(x)      # Jo≈° jedna linearna transformacija\n","        x = self.activation2(x)  # Jo≈° jedan ReLU\n","        x = self.layer3(x)      # Finalni linearni sloj (bez aktivacije)\n","        return x\n","\n","    def get_info(self):\n","        \"\"\"Vrati informacije o mre≈æi.\"\"\"\n","        total_params = sum(p.numel() for p in self.parameters())\n","        return f\"CustomNetwork: {self.architecture}, Parameters: {total_params}\"\n","\n","# Izgradi na≈°u prilagoƒëenu mre≈æu\n","custom_network = CustomNetwork()\n","print(\"=== Custom Neural Network Class ===\")\n","print(custom_network.get_info())\n","print(f\"Network architecture:\\n{custom_network}\")"],"metadata":{"id":"MXmvcIKUVl6j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Poreƒëenje Prilagoƒëene Mre≈æe i `nn.Sequential` Mre≈æe"],"metadata":{"id":"3Iw_tIe-WDny"}},{"cell_type":"code","source":["# Poredi sa nn.Sequential verzijom\n","sequential_network = nn.Sequential(\n","    nn.Linear(4, 8),\n","    nn.ReLU(),\n","    nn.Linear(8, 4),\n","    nn.ReLU(),\n","    nn.Linear(4, 1)\n",")\n","\n","print(\"\\nSequential network architecture:\")\n","print(sequential_network)\n","\n","# Kopiraj te≈æine da mre≈æe budu identiƒçne za poreƒëenje\n","custom_layers = [custom_network.layer1, custom_network.layer2, custom_network.layer3]\n","sequential_layers = [sequential_network[0], sequential_network[2], sequential_network[4]]\n","\n","for custom_layer, seq_layer in zip(custom_layers, sequential_layers):\n","    custom_layer.weight.data = seq_layer.weight.data.clone()\n","    custom_layer.bias.data = seq_layer.bias.data.clone()\n","\n","# Testiraj obe mre≈æe sa istim ulazom\n","test_input = torch.randn(5, 4)\n","custom_output = custom_network(test_input)\n","sequential_output = sequential_network(test_input)\n","\n","print(f\"\\nTesting both approaches:\")\n","print(f\"Input shape: {test_input.shape}\")\n","print(f\"Custom network output shape: {custom_output.shape}\")\n","print(f\"Sequential network output shape: {sequential_output.shape}\")\n","print(f\"Outputs are identical: {torch.allclose(custom_output, sequential_output)}\")\n","print(f\"Max difference: {torch.abs(custom_output - sequential_output).max():.10f}\")\n","\n","# Poreƒëenje parametara\n","custom_params = sum(p.numel() for p in custom_network.parameters())\n","sequential_params = sum(p.numel() for p in sequential_network.parameters())\n","\n","print(f\"\\nParameter count:\")\n","print(f\"Custom network: {custom_params}\")\n","print(f\"Sequential network: {sequential_params}\")\n","print(f\"Same parameter count: {custom_params == sequential_params}\")"],"metadata":{"id":"uXbL4eVgWKfI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"tiixQeX0UGP-"}},{"cell_type":"markdown","source":["# **Deo 4: PyTorch Optimizatori**"],"metadata":{"id":"LzGHLv2BqHMW"}},{"cell_type":"markdown","source":["## 4.1 ≈†ta su Optimizatori?\n","\n","U Delu 3, nauƒçili smo kako da gradimo neuronske mre≈æe koje mogu da prave predviƒëanja. Ali kako one zapravo **uƒçe** iz podataka da naprave **taƒçna predviƒëanja**? E, tu dolaze optimizatori!\n","\n","**Optimizator je kao trener** koji govori mre≈æi kako da pobolj≈°a svoje performanse. Posmatrajte ovo na sledeƒái naƒçin:\n","- Va≈°a mre≈æa pravi predviƒëanja (neka dobra, neka lo≈°a)\n","- Funkcija gubitka meri koliko su predviƒëanja pogre≈°na\n","- Optimizator koristi ovo merenje da shvati kako da prilagodi parametre mre≈æe da pravi bolja predviƒëanja\n","\n","U analogiji sa fizikom, zamislite da poku≈°avate da naƒëete najni≈æu taƒçku na potencijalnoj povr≈°i (minimum gubitka). Optimizator je va≈°a strategija za efikasno hodanje nizbrdo. Najƒçe≈°ƒáe je to neka vrsta **gradijentnog spusta**."],"metadata":{"id":"yR0W39n4qZEF"}},{"cell_type":"code","source":["# Uƒçitajmo PyTorch ponovo, ali ovaj put se uverimo da ukljuƒçimo torch.optim kao optim\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Postavi random seed za reproducibilnost\n","torch.manual_seed(42)\n","print(\"PyTorch version:\", torch.__version__)"],"metadata":{"id":"uMRRhmCiqmeX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Razumevanje Gradijentnog Spusta"],"metadata":{"id":"y18tIdq7q11o"}},{"cell_type":"code","source":["# Defini≈°i na≈°u jednostavnu 1D funkciju gubitka\n","def simple_loss_function(x):\n","    \"\"\"Jednostavna kvadratna funkcija gubitka: f(x) = (x - 2)^2 + 1\"\"\"\n","    return (x - 2) ** 2 + 1\n","\n","def gradient_of_loss(x):\n","    \"\"\"Gradijent (izvod) na≈°e funkcije gubitka: f'(x) = 2(x - 2)\"\"\"\n","    return 2 * (x - 2)\n","\n","# Kreiraj taƒçke podataka za vizualizaciju\n","x_range = torch.linspace(-1, 5, 100)\n","loss_values = simple_loss_function(x_range)\n","\n","# Poƒçetna taƒçka za na≈° \"parametar\"\n","x_current = torch.tensor(4.0, requires_grad=True)\n","\n","# Kreiraj poƒçetnu vizualizaciju\n","plt.figure(figsize=(15, 5))\n","\n","# Plot 1: Predeo gubitka (Loss landscape)\n","plt.subplot(1, 3, 1)\n","plt.plot(x_range.numpy(), loss_values.numpy(), 'b-', linewidth=2, label='Loss Function')\n","plt.axvline(x=2.0, color='r', linestyle='--', alpha=0.7, label='True Minimum (x=2)')\n","\n","# Koristi .item() da dobije≈° skalarnu vrednost za crtanje\n","plt.scatter([x_current.item()], [simple_loss_function(x_current).item()],\n","           color='orange', s=100, label=f'Starting Point (x={x_current.item():.1f})')\n","plt.xlabel('Parameter Value (x)')\n","plt.ylabel('Loss Value')\n","plt.title('Loss Landscape')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","# Sada hajde da izvr≈°imo gradijentni spust\n","learning_rate = 0.1\n","num_steps = 10\n","history = []\n","\n","# ƒåuvaj poƒçetno stanje\n","with torch.no_grad():  # Koristi no_grad da izbegne≈° graƒëenje computation graph-a\n","    history.append((x_current.item(), simple_loss_function(x_current).item()))\n","\n","print(\"Gradient Descent Steps:\")\n","print(f\"Step 0: x = {x_current.item():.4f}, Loss = {simple_loss_function(x_current).item():.4f}\")\n","\n","for step in range(num_steps):\n","    # Izraƒçunaj gubitak\n","    loss = simple_loss_function(x_current)\n","\n","    # Izraƒçunaj gradijent\n","    loss.backward()  # Ovo raƒçuna gradijent\n","\n","    # A≈æuriraj parametar koristeƒái gradijentni spust\n","    with torch.no_grad():\n","        x_current -= learning_rate * x_current.grad\n","        # ƒåuvaj istoriju za crtanje\n","        history.append((x_current.item(), simple_loss_function(x_current).item()))\n","        print(f\"Step {step+1}: x = {x_current.item():.4f}, Loss = {simple_loss_function(x_current).item():.4f}, Gradient = {x_current.grad.item():.4f}\")\n","\n","    # Postavi gradijent na nulu za sledeƒáu iteraciju\n","    x_current.grad.zero_()\n","\n","# Plot 2: Progres optimizacije\n","plt.subplot(1, 3, 2)\n","steps = list(range(len(history)))\n","losses = [h[1] for h in history]\n","plt.plot(steps, losses, 'go-', linewidth=2, markersize=6)\n","plt.xlabel('Optimization Step')\n","plt.ylabel('Loss Value')\n","plt.title('Loss During Optimization')\n","plt.grid(True, alpha=0.3)\n","\n","# Plot 3: Putanja parametra na predelu gubitka\n","plt.subplot(1, 3, 3)\n","plt.plot(x_range.numpy(), loss_values.numpy(), 'b-', linewidth=2, alpha=0.7, label='Loss Function')\n","plt.axvline(x=2.0, color='r', linestyle='--', alpha=0.7, label='True Minimum')\n","\n","# Nacrtaj putanju koju je pro≈°ao gradijentni spust\n","x_positions = [h[0] for h in history]\n","y_positions = [h[1] for h in history]\n","plt.plot(x_positions, y_positions, 'go-', linewidth=2, markersize=6, alpha=0.8, label='GD Path')\n","plt.scatter([x_positions[0]], [y_positions[0]], color='orange', s=100, label='Start', zorder=5)\n","plt.scatter([x_positions[-1]], [y_positions[-1]], color='red', s=100, label='End', zorder=5)\n","\n","plt.xlabel('Parameter Value (x)')\n","plt.ylabel('Loss Value')\n","plt.title('Gradient Descent Path')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Rezime\n","print(f\"\\nSummary:\")\n","print(f\"Started at x = {history[0][0]:.4f} with loss = {history[0][1]:.4f}\")\n","print(f\"Ended at x = {history[-1][0]:.4f} with loss = {history[-1][1]:.4f}\")\n","print(f\"True minimum is at x = 2.0000 with loss = 1.0000\")\n","print(f\"Error from true minimum: {abs(history[-1][0] - 2.0):.4f}\")"],"metadata":{"id":"Mgnyfd6vq2Uz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Vizuelizacija Efekata Stope Uƒçenja"],"metadata":{"id":"tHsO800cq4Wz"}},{"cell_type":"code","source":["# Simuliraj gradijentni spust sa razliƒçitim stopama uƒçenja\n","def gradient_descent_simulation(learning_rate, num_steps=15):\n","    \"\"\"Simuliraj gradijentni spust na na≈°oj jednostavnoj funkciji\"\"\"\n","    x = 4  # Poƒçetna taƒçka\n","    history = [x]\n","    loss_history = [simple_loss_function(torch.tensor(x)).item()]\n","\n","    for step in range(num_steps):\n","        gradient = gradient_of_loss(torch.tensor(x))\n","        x = x - learning_rate * gradient.item()\n","        history.append(x)\n","        loss_history.append(simple_loss_function(torch.tensor(x)).item())\n","\n","    return history, loss_history\n","\n","# Testiraj razliƒçite stope uƒçenja\n","learning_rates = [0.1, 0.3, 1.0, 1.03]\n","colors = ['blue', 'green', 'red', 'purple']\n","\n","plt.figure(figsize=(14, 5))\n","\n","plt.subplot(1, 2, 1)\n","# Nacrtaj funkciju gubitka\n","x_range = torch.linspace(-1, 5, 100)\n","loss_values = simple_loss_function(x_range)\n","plt.plot(x_range.numpy(), loss_values.numpy(), 'k-', linewidth=2, alpha=0.3, label='Loss Function')\n","\n","for lr, color in zip(learning_rates, colors):\n","    x_hist, loss_hist = gradient_descent_simulation(lr)\n","    # Nacrtaj putanju na funkciji gubitka\n","    x_points = torch.tensor(x_hist)\n","    y_points = simple_loss_function(x_points)\n","    plt.plot(x_points.numpy(), y_points.numpy(), 'o-', color=color, alpha=0.7,\n","            markersize=4, label=f'LR = {lr}')\n","\n","plt.axvline(x=2.0, color='r', linestyle='--', alpha=0.5, label='True Minimum')\n","plt.xlabel('Parameter Value')\n","plt.ylabel('Loss')\n","plt.title('Gradient Descent Paths')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","plt.subplot(1, 2, 2)\n","for lr, color in zip(learning_rates, colors):\n","    x_hist, loss_hist = gradient_descent_simulation(lr)\n","    plt.plot(loss_hist, 'o-', color=color, alpha=0.7, markersize=4, label=f'LR = {lr}')\n","\n","plt.xlabel('Optimization Step')\n","plt.ylabel('Loss Value')\n","plt.title('Loss vs Training Steps')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"Efekti razliƒçitih stopa uƒçenja:\")\n","print(\"üîµ LR = 0.1: Spor ali stabilan napredak\")\n","print(\"üü¢ LR = 0.5: Dobra ravnote≈æa brzine i stabilnosti\")\n","print(\"üî¥ LR = 1.0: Na granici stabilnosti\")\n","print(\"üü£ LR = 1.03: Previsoka! Oscilacije i nestabilnost\")"],"metadata":{"id":"jKFgCAdBq6XC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"ulu4nrMHrAQj"}},{"cell_type":"markdown","source":["## 4.2 Najƒçe≈°ƒáe Kori≈°ƒáeni Optimizatori\n","\n","Da biste pristupili ugraƒëenim PyTorch optimizatorima, ne zaboravite da uradite `import torch.optim as optim`. Ako ≈æelite, mo≈æete takoƒëe napraviti prilagoƒëeni optimizator, ali to neƒáemo raditi na ovom kursu.\n","\n","Pogledajmo najpopularnije optimizatore u PyTorch-u:\n","*   SGD (Stochastic Gradient Descent)\n","*   Adam (Adaptive Moment Estimation) - Veoma popularan!\n","*   RMSprop"],"metadata":{"id":"f-eKbP4crBR7"}},{"cell_type":"code","source":["# Kreiraj jednostavan model za demonstraciju optimizatora\n","class SimpleModel(nn.Module):\n","    def __init__(self):\n","        super(SimpleModel, self).__init__()\n","        self.linear = nn.Linear(1, 1)\n","\n","    def forward(self, x):\n","        return self.linear(x)\n","\n","# Kreiraj model i neke la≈æne podatke\n","model = SimpleModel()\n","x_data = torch.randn(10, 1)\n","y_data = torch.randn(10, 1)\n","\n","print(\"=== Common Optimizers ===\")\n","\n","# 1. SGD (Stochastic Gradient Descent)\n","sgd_optimizer = optim.SGD(model.parameters(), lr=0.01)\n","print(f\"SGD: {sgd_optimizer}\")\n","\n","# 2. Adam (Adaptive Moment Estimation)\n","adam_optimizer = optim.Adam(model.parameters(), lr=0.01)\n","print(f\"Adam: {adam_optimizer}\")\n","\n","# 3. RMSprop\n","rmsprop_optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n","print(f\"RMSprop: {rmsprop_optimizer}\")\n","\n","print(f\"\\nKey parameters:\")\n","print(f\"- lr (learning rate): Koliko veliki koraci da se prave\")\n","print(f\"- Veƒáina optimizatora ima dodatne parametre za fino pode≈°avanje\")"],"metadata":{"id":"TzASvR6BrGAY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"r4wM9ahOWPGu"}},{"cell_type":"markdown","source":["# Kljuƒçni Zakljuƒçci"],"metadata":{"id":"Cf-it3zYWQk7"}},{"cell_type":"markdown","source":["## ≈†ta smo nauƒçili:\n","\n","1. **Univerzalni obrazac**: Gradivni blokovi u PyTorch-u (slojevi, aktivacije, gubici) su podklase osnovne `nn.Module` klase\n","2. **Ugraƒëeni moduli**: Veƒá napravljene komponente koje prate konzistentne interfejse\n","3. **Prilagoƒëeni moduli**: Kako da izgradite svoje komponente od nule\n","4. **Kompozicija modula**: Kako da kombinujete module u veƒáe sisteme\n","5. **Ispod haube**: ≈†ta se stvarno de≈°ava unutar ƒçestih PyTorch komponenti\n","6. **Optimizatori**: Algoritmi koji ƒçine da neuronske mre≈æe uƒçe"],"metadata":{"id":"97kVr3MUWVU-"}},{"cell_type":"markdown","source":["## PyTorch filozofija:\n","\n","- **Modularnost**: Mali, iznova upotrebljivi delovi\n","- **Konzistentnost**: Isti interfejs za sve komponente\n","- **Transparentnost**: Lako je videti i menjati ≈°ta se de≈°ava unutra\n","- **Kompozibilnost**: Jednostavni delovi se kombinuju da naprave slo≈æene sisteme"],"metadata":{"id":"4r39EglRXqdl"}},{"cell_type":"markdown","source":["## Sledeƒái koraci:\n","\n","U sledeƒáem odeljku, nauƒçiƒáemo o **Radnim tokovima u PyTorch-u** - kako da stvarno sve spojimo u jednu celinu. Koristiƒáemo module koje sada razumemo i dodati finalne delove potrebne za ma≈°insko uƒçenje."],"metadata":{"id":"rp0KouMMXsHU"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"PR046h-4Wl-6"}},{"cell_type":"markdown","source":["# Samostalni Rad\n","\n","Zapamtite, svaki prilagoƒëeni modul treba da ima:\n","1. super().**__init__**() u **init**\n","2. nn.Parameter() ako ima parametre koji se mogu uƒçiti\n","3. forward() metodu za izraƒçunavanje"],"metadata":{"id":"-IpRZ1zTWp87"}},{"cell_type":"markdown","source":["## Zadaci\n","\n","1. Kreirajte prilagoƒëenu Sigmoid aktivaciju\n","    - Savet: sigmoid(x) = 1 / (1 + exp(-x))\n","    - Koristite torch.exp() i pazite na numeriƒçku stabilnost\n","2. Kreirajte prilagoƒëenu L1Loss (Mean Absolute Error) funkciju gubitka\n","    - Savet: L1(prediction, target) = mean(|prediction - target|)\n","    - Koristite torch.abs()\n","3. Kreirajte prilagoƒëeni sloj koji dodaje \"uƒçljiv\" pomeraj  **b** bilo kom ulazu **f(x) = x + b**\n","    - Ovaj sloj treba da doda razliƒçit pomeraj svakoj karakteristici ulaznih podataka\n","4. Kombinujte svoje prilagoƒëene module\n","    - Izgradite mre≈æu koristeƒái va≈°u prilagoƒëenu Sigmoid i L1Loss\n","    - Izgradite je sa PyTorch-ovim ugraƒëenim ekvivalentima"],"metadata":{"id":"wQKd-RRmWr3J"}},{"cell_type":"code","source":["# Mesto za re≈°avanje Zadatka 1"],"metadata":{"id":"Fe8lv3RdXhwm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mesto za re≈°avanje Zadatka 2"],"metadata":{"id":"wJ_guypzXho1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mesto za re≈°avanje Zadatka 3"],"metadata":{"id":"zN_sHwW-Xhhq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mesto za re≈°avanje Zadatka 4"],"metadata":{"id":"Q9ySmxY3XhR1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"wrZ2jsWExUK4"}},{"cell_type":"markdown","source":["## Re≈°enja"],"metadata":{"id":"Fp9BL0awxU4-"}},{"cell_type":"code","source":["# -------- Zadatak 1 --------\n","import torch\n","import torch.nn as nn\n","\n","class CustomSigmoid(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        # Numeriƒçki stabilna sigmoid funkcija:\n","        # Za x >= 0: sigmoid(x) = 1 / (1 + exp(-x))\n","        # Za x < 0 : sigmoid(x) = exp(x) / (1 + exp(x))\n","        out = torch.empty_like(x)\n","        pos_mask = (x >= 0)\n","        neg_mask = ~pos_mask\n","\n","        # Stabilno izraƒçunavanje za pozitivne vrednosti\n","        out[pos_mask] = 1 / (1 + torch.exp(-x[pos_mask]))\n","\n","        # Stabilno izraƒçunavanje za negativne vrednosti\n","        exp_x = torch.exp(x[neg_mask])\n","        out[neg_mask] = exp_x / (1 + exp_x)\n","\n","        return out"],"metadata":{"id":"ZpDhNRQjxXG1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------- Zadatak 2 --------\n","import torch\n","import torch.nn as nn\n","\n","class CustomL1Loss(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, pred, target):\n","        return torch.mean(torch.abs(pred - target))\n"],"metadata":{"id":"P3gCKtUEx200"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------- Zadatak 3 --------\n","import torch\n","import torch.nn as nn\n","\n","class BiasLayer(nn.Module):\n","    \"\"\"\n","    Dodaje uƒçljiv pomeraj svakoj karakteristici: f(x) = x + b\n","    Radi za ulaze oblika (batch_size, num_features).\n","    \"\"\"\n","    def __init__(self, num_features):\n","        super().__init__()\n","        # bias (pomeraj) je uƒçljiv parametar oblika (num_features,)\n","        self.bias = nn.Parameter(torch.zeros(num_features))\n","\n","    def forward(self, x):\n","        # Broadcasting automatski dodaje bias svakom uzorku u batch-u\n","        return x + self.bias"],"metadata":{"id":"-87wLKuax9zn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------- Zadatak 4 --------\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","torch.manual_seed(0)\n","N, D_in, D_out = 512, 10, 1\n","\n","# ---- Dva modela: (A) prilagoƒëen, (B) ugraƒëen ----\n","class TinyNetCustom(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.lin = nn.Linear(D_in, D_out, bias=False)\n","        self.bias = BiasLayer(D_out)        # uƒçljiv bias\n","        self.act = CustomSigmoid()          # prilagoƒëena sigmoid funkcija\n","\n","    def forward(self, x):\n","        return self.act(self.bias(self.lin(x)))\n","\n","class TinyNetBuiltin(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.lin = nn.Linear(D_in, D_out, bias=False)\n","        self.bias = BiasLayer(D_out)\n","\n","    def forward(self, x):\n","        return torch.sigmoid(self.bias(self.lin(x)))\n","\n","netA = TinyNetCustom()\n","netB = TinyNetBuiltin()\n","\n","# Inicijalizuj obe mre≈æe sa istim te≈æinama za ravnopravno poreƒëenje\n","with torch.no_grad():\n","    netB.lin.weight.copy_(netA.lin.weight)\n","    netB.bias.bias.copy_(netA.bias.bias)\n","\n","lossA = CustomL1Loss()\n","lossB = nn.L1Loss()\n","\n","optA = optim.SGD(netA.parameters(), lr=0.5)\n","optB = optim.SGD(netB.parameters(), lr=0.5)"],"metadata":{"id":"dppyqJn5yNgM"},"execution_count":null,"outputs":[]}]}